{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "981664df",
   "metadata": {},
   "source": [
    "# ***Prepoznavanje imenovanih entiteta (NER)***"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe21e6e6",
   "metadata": {},
   "source": [
    "### Opis: Ispitajte algoritme koji identificiraju imena, lokacije i organizacije unutar teksta."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd3f6baf",
   "metadata": {},
   "source": [
    "##### Molim vas da stavite NER.ipynb u folder/mapu i pokrenete ƒáeliju ispod tako da se svi potrebni podaci skinu (oko 1.01 GB)\n",
    "* U sluƒçaju da imate kaggle raƒçun veƒá spojen jedna od ƒáelija se mo≈æe preskoƒçiti"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18cf3722",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Potrebno za sve celije ispod\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5493314c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "\n",
    "notebook_path = Path().resolve()  # fallback if metadata fails\n",
    "try:\n",
    "    from notebook import notebookapp\n",
    "    import requests, json\n",
    "    # Pronalazenje trenutnog puta i imena foldera u kojem se nalazi\n",
    "    connection_file = os.path.basename(os.path.realpath(os.path.join(os.getcwd(), '..', 'kernel.json')))\n",
    "except:\n",
    "    pass\n",
    "\n",
    "# Put do 'NER.ipynb'\n",
    "notebook_path = Path(\"NER.ipynb\").resolve().parent\n",
    "\n",
    "# Kreiranje 'data' foldera gdje se nalazi 'NER.ipynb'\n",
    "dest_dir = notebook_path / \"data\"\n",
    "os.makedirs(dest_dir, exist_ok=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77105588",
   "metadata": {},
   "outputs": [],
   "source": [
    "import io, zipfile, urllib.request, shutil\n",
    "\n",
    "# === Settings ===\n",
    "repo = \"tinficok-faks/NER\"   # owner/repo\n",
    "branch = \"main\"              # branch name\n",
    "subfolder = \"data\"           # the subfolder you want from the repo\n",
    "\n",
    "# === Download the ZIP of the repo ===\n",
    "zip_url = f\"https://github.com/{repo}/archive/refs/heads/{branch}.zip\"\n",
    "with urllib.request.urlopen(zip_url) as resp:\n",
    "    zip_bytes = resp.read()\n",
    "\n",
    "# === Extract ONLY the subfolder ===\n",
    "with zipfile.ZipFile(io.BytesIO(zip_bytes)) as z:\n",
    "    # GitHub zips have a top-level folder like \"NER-main/\"\n",
    "    names = z.namelist()\n",
    "    if not names:\n",
    "        raise RuntimeError(\"Empty zip from GitHub.\")\n",
    "    top = names[0].split(\"/\")[0]  # npr. 'NER-main'\n",
    "    prefix = f\"{top}/{subfolder.strip('/')}/\"\n",
    "\n",
    "    found_any = False\n",
    "    for member in names:\n",
    "        if member.startswith(prefix) and not member.endswith(\"/\"):\n",
    "            found_any = True\n",
    "            rel_path = member[len(prefix):]  # put do mog subfoldera\n",
    "            target_path = os.path.join(dest_dir, rel_path)\n",
    "            os.makedirs(os.path.dirname(target_path), exist_ok=True)\n",
    "            with z.open(member) as src, open(target_path, \"wb\") as dst:\n",
    "                shutil.copyfileobj(src, dst)\n",
    "\n",
    "if not found_any:\n",
    "    raise FileNotFoundError(f\"Subfolder '{subfolder}' not found in repo {repo}@{branch}.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c6404784",
   "metadata": {},
   "source": [
    "üü® ƒÜelija ispod se mo≈æe preskoƒçiti ako imate spojen config datoteku s API-jem veƒá nekog Kaggle raƒçuna"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9f76d04",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install kaggle\n",
    "\n",
    "# Target file path\n",
    "dest_file = notebook_path / \"kaggle.json\"\n",
    "\n",
    "# Download the file\n",
    "url = \"https://raw.githubusercontent.com/tinficok-faks/NER/main/kaggle.json\"\n",
    "urllib.request.urlretrieve(url, dest_file)\n",
    "\n",
    "print(f\"Downloaded kaggle.json to: {dest_file}\")\n",
    "\n",
    "# Ovo je moj API key\n",
    "os.environ[\"KAGGLE_CONFIG_DIR\"] = os.getcwd()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "817af2bb",
   "metadata": {},
   "source": [
    "üü© Ova ƒáelija je potrebna za preuzimanje baze gradova i kompanija"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd7f3a71",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Download i unzip u 'data' folder\n",
    "!kaggle datasets download -d juanmah/world-cities -p \"data\" --unzip\n",
    "!kaggle datasets download -d peopledatalabssf/free-7-million-company-dataset -p \"data\" --unzip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69ad8ad5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Za brisanje 'kaggle.json' jer nije potreban\n",
    "# A ako si ga zelite zadrzati mozete obrisati/zakomentirati ovo\n",
    "file_to_delete = notebook_path / \"kaggle.json\"\n",
    "if file_to_delete.exists():\n",
    "    file_to_delete.unlink()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa68effe",
   "metadata": {},
   "source": [
    "# Poƒçetak NER programa"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "56f02cab",
   "metadata": {},
   "source": [
    "Sve potrebne biblioteke za projekt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "715fccaf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Required libraries\n",
    "import os\n",
    "import random\n",
    "import nltk\n",
    "import regex as re\n",
    "import pandas as pd\n",
    "from nltk import word_tokenize\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn_crfsuite import CRF\n",
    "from sklearn_crfsuite.metrics import flat_classification_report\n",
    "\n",
    "# Download NLTK tokenizer\n",
    "nltk.download('punkt')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "011de165",
   "metadata": {},
   "source": [
    "Ucitavanje podataka\n",
    "* imena\n",
    "* gradovi\n",
    "* kompanije\n",
    "* dr≈æave"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73b10ead",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ucitavanje HR i ENG imena iz 4 text file-a\n",
    "def load_names():\n",
    "    names = set()\n",
    "    files = ['ENG-muska-imena.txt', 'ENG-zenska-imena.txt', 'HR-muska-imena.txt', 'HR-zenska-imena.txt']\n",
    "    for file in files:\n",
    "        path = os.path.join('data', file)\n",
    "        with open(path, 'r', encoding='utf-8') as f:\n",
    "            for line in f:\n",
    "                names.add(line.strip().lower())\n",
    "    return names\n",
    "\n",
    "# Ucitavanje gradova iz excel baze\n",
    "def load_cities():\n",
    "    df = pd.read_csv('data/worldcities.csv')\n",
    "    return set(df['city'].dropna().str.lower().unique())\n",
    "\n",
    "# Ucitavanje 7M kompanija iz excel baze\n",
    "def load_companies():\n",
    "    df = pd.read_csv('data/companies_sorted.csv', usecols=['name'], encoding='utf-8', low_memory=False)\n",
    "    return set(df['name'].dropna().str.lower().unique())\n",
    "\n",
    "# Ucitavanje drzava na HR i ENG iz text file-ova\n",
    "def load_countries():\n",
    "    countries = set()\n",
    "    files = ['countries.txt', 'drzave.txt']\n",
    "    for file in files:\n",
    "        path = os.path.join('data', file)\n",
    "        with open(path, 'r', encoding='utf-8') as f:\n",
    "            for line in f:\n",
    "                line = line.strip()\n",
    "                if line:  # skip empty lines\n",
    "                    countries.add(line.lower())\n",
    "    return countries\n",
    "\n",
    "names_set = load_names()\n",
    "cities_set = load_cities()\n",
    "companies_set = load_companies()\n",
    "countries_set = load_countries()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6822964e",
   "metadata": {},
   "source": [
    "Tokenizira reƒçenice i pretvara ih u BIO (beggining, inside, out) oznake"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2da67e87",
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_data(sentences):\n",
    "    data = []\n",
    "    for sent, annotations in sentences:\n",
    "        tokens = word_tokenize(sent)\n",
    "        labels = ['O'] * len(tokens)\n",
    "        for entity, tag in annotations:\n",
    "            entity_tokens = word_tokenize(entity)\n",
    "            for i in range(len(tokens) - len(entity_tokens) + 1):\n",
    "                if tokens[i:i+len(entity_tokens)] == entity_tokens:\n",
    "                    labels[i] = 'B-' + tag\n",
    "                    for j in range(1, len(entity_tokens)):\n",
    "                        labels[i + j] = 'I-' + tag\n",
    "                    break\n",
    "        data.append((tokens, labels))\n",
    "    return data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a7c31ef4",
   "metadata": {},
   "source": [
    "* **word2features** - pravi rjeƒçnik za treniranje modela kao ≈°to je CRF, koristi posebne znakove za BOS (beginning of sentence) i EOS (end of sentence)\n",
    "* **sent2features** - pokreƒáe funkciju *word2features* za svaki token u reƒçenici\n",
    "* **sent2labels** - vraƒáa nepromijenjenu listu oznaka/tagova"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3c0e8cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def word2features(sent, i):\n",
    "    word = sent[i]\n",
    "    features = {\n",
    "        'bias': 1.0,\n",
    "        'word.lower()': word.lower(),\n",
    "        'word.istitle()': word.istitle(),\n",
    "        'word.isupper()': word.isupper(),\n",
    "        'word.isdigit()': word.isdigit(),\n",
    "        'suffix3': word[-3:],\n",
    "        'prefix3': word[:3],\n",
    "        'in_names': word.lower() in names_set,\n",
    "        'in_cities': word.lower() in cities_set,\n",
    "        'in_companies': word.lower() in companies_set,\n",
    "    }\n",
    "    if i > 0:\n",
    "        features.update({\n",
    "            '-1:word.lower()': sent[i-1].lower(),\n",
    "            '-1:word.istitle()': sent[i-1].istitle(),\n",
    "        })\n",
    "    else:\n",
    "        features['BOS'] = True\n",
    "\n",
    "    if i < len(sent) - 1:\n",
    "        features.update({\n",
    "            '+1:word.lower()': sent[i+1].lower(),\n",
    "            '+1:word.istitle()': sent[i+1].istitle(),\n",
    "        })\n",
    "    else:\n",
    "        features['EOS'] = True\n",
    "\n",
    "    return features\n",
    "\n",
    "def sent2features(sent):\n",
    "    return [word2features(sent, i) for i in range(len(sent))]\n",
    "\n",
    "def sent2labels(labels):\n",
    "    return labels"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf9c998d",
   "metadata": {},
   "source": [
    "Uƒçitavanje podataka za treniranje\n",
    "* train_1000 - ima 1000 reƒçenica\n",
    "* train_2000 - ima 2000 reƒçenica\n",
    "* train_7000 - ima 7000 reƒçenica"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2f2f1cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# train test\n",
    "def load_training_data(filepath):\n",
    "    sentences = []\n",
    "    tokens = []\n",
    "    labels = []\n",
    "    \n",
    "    with open(filepath, 'r', encoding='utf-8') as f:\n",
    "        for line in f:\n",
    "            line = line.strip()\n",
    "            if not line:\n",
    "                if tokens:\n",
    "                    sentences.append((tokens, labels))\n",
    "                    tokens = []\n",
    "                    labels = []\n",
    "                continue\n",
    "            try:\n",
    "                token, label = line.split()\n",
    "                tokens.append(token)\n",
    "                labels.append(label)\n",
    "            except ValueError:\n",
    "                print(f\"Skipping malformed line: {line}\")\n",
    "    \n",
    "    if tokens:\n",
    "        sentences.append((tokens, labels))\n",
    "    \n",
    "    return sentences\n",
    "\n",
    "# Load the dataset\n",
    "# data = load_training_data('data/train_1000.txt')\n",
    "# data = load_training_data('data/train_2000.txt')\n",
    "data = load_training_data('data/train_7000.txt')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "afba66a7",
   "metadata": {},
   "source": [
    "Slijed dogaƒëaja\n",
    "* Randomiziranje reƒçenica kako bi se izbjegla pristranost\n",
    "* X - popis niza znaƒçajki za svaku reƒçenicu iz *sent2features*\n",
    "* Y - popis nizova oznaka za svaku reƒçenicu\n",
    "* Podjela na 80% treniranje i 20% testiranje\n",
    "* *Train CRF* - kreira Conditional Random Field i prilagoƒëava ga oznakama za uƒçenje"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47070a12",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare training data\n",
    "random.shuffle(data)\n",
    "\n",
    "X = [sent2features(tokens) for tokens, _ in data]\n",
    "y = [labels for _, labels in data]\n",
    "\n",
    "\n",
    "# Train/Test Split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2)\n",
    "\n",
    "# Train CRF model\n",
    "crf = CRF(algorithm='lbfgs', max_iterations=1000)\n",
    "crf.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "61d37dce",
   "metadata": {},
   "source": [
    "Provjerava koliko se dobro predviƒëene oznake modela podudaraju s ispravnim oznakama nevidljivih reƒçenica"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44bb46b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate model\n",
    "y_pred = crf.predict(X_test)\n",
    "print(flat_classification_report(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a16a277",
   "metadata": {},
   "source": [
    "Funkcija uzima tekst i vraƒáa entitete (B,I,O) koje pronaƒëe istrenirani CRF model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7bc6bd5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_entities(text):\n",
    "    tokens = word_tokenize(text)\n",
    "    feats = sent2features(tokens)\n",
    "    preds = crf.predict_single(feats)\n",
    "    entities = []\n",
    "    current = []\n",
    "    current_tag = None\n",
    "\n",
    "    for token, label in zip(tokens, preds):\n",
    "        if label.startswith(\"B-\"):\n",
    "            if current:\n",
    "                entities.append((\" \".join(current), current_tag))\n",
    "            current = [token]\n",
    "            current_tag = label[2:]\n",
    "        elif label.startswith(\"I-\") and current:\n",
    "            current.append(token)\n",
    "        else:\n",
    "            if current:\n",
    "                entities.append((\" \".join(current), current_tag))\n",
    "                current = []\n",
    "                current_tag = None\n",
    "    if current:\n",
    "        entities.append((\" \".join(current), current_tag))\n",
    "    return entities"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "82828eed",
   "metadata": {},
   "source": [
    "Ruƒçno provjeravanje pozivanjem funkcije 'predict_entities' ≈°to ƒáe model izbaciti"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e745a13d",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(predict_entities(\"Ivan radi za Rimac Automobili i ≈æivi u Splitu u Hrvatskoj\"))\n",
    "print(predict_entities(\"Marla Vidakoviƒá works for Microsoft in London.\"))\n",
    "print(predict_entities(\"Tihomira Biliƒáa nema na igrali≈°tu pored Adrie\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a39019c2",
   "metadata": {},
   "source": [
    "Pokretanje NER modela na svim reƒçenicama u fileovima\n",
    "* test ima 98 reƒçenica\n",
    "* test_veliki ima 2000 reƒçenica"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ccc4cea",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run NER on every sentence in test files\n",
    "# test_file = 'data/test.txt'\n",
    "test_file = 'data/test_veliki.txt'\n",
    "\n",
    "with open(test_file, 'r', encoding='utf-8') as f:\n",
    "    test_sentences = [line.strip() for line in f if line.strip()]\n",
    "\n",
    "print(f\"Loaded {len(test_sentences)} test sentences\\\\n\")\n",
    "\n",
    "for i, sent in enumerate(test_sentences, 1):\n",
    "    entities = predict_entities(sent)\n",
    "    print(f\"{i:>3}. {sent}\")\n",
    "    for ent, tag in entities:\n",
    "        print(f\"    -> {ent:25}  [{tag}]\")\n",
    "    print()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
